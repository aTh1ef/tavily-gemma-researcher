# ğŸ” Tavily-Gemma Research Helper

**Tavilyâ€‘Gemma Research Helper** is a smart, localâ€‘first research assistant that helps you analyze any topic with realâ€‘time web data and stepâ€‘byâ€‘step research guidance powered by LangGraph.

Itâ€™s more than just a summarizerâ€”itâ€™s a full AI research workflow designed to teach critical thinking and show how insights are generated.

This app helps you understand how to approach a topic from multiple angles. It gives you initial leads, suggests exploratory paths, and offers different lenses through which to run your brain around the subject. By using this tool regularly, you'll develop a sharper research mindset and learn to think critically and creatively about any topic you explore.
Over time, it turns research from a task into a skill â€” and curiosity into a method.

---

## ğŸ” What It Does

This app helps you:

* Plan research workflows using a LangGraph agent
* Query a **local LLM** (Gemma 3B) via LM Studio
* Pull live search results using the Tavily API
* Review a clean, transparent research output (with reasoning path!)

---

## ğŸ§  Techâ€‘Powered Workflow

Each part of the app is purposeâ€‘built for clarity and flexibility:

### ğŸ§­ Research Flow Orchestration (LangGraph)

LangGraph powers the structured research pipeline: planning â†’ searching â†’ synthesis.

### ğŸ¤– Local Reasoning via LM Studio (google/gemma-3-1b)

**LLM used:** `google/gemma-3-1b`, served via LM Studioâ€™s OpenAIâ€‘compatible API mode.
Why this model? Itâ€™s fast, instructionâ€‘tuned, and excels at structured thought processes.

### ğŸŒ Search & Data via Tavily API

Live web results come from Tavily Search, curated and formatted into sourceâ€‘rich summaries.

---

## ğŸ› ï¸ Installation & Setup

### Step 1: Clone the Repository

```bash
git clone https://github.com/aTh1ef/tavily-gemma-researcher.git
cd tavily-gemma-researcher
```

### Step 2: Install Dependencies

Ensure you have **Python 3.10+**, then:

```bash
pip install -r requirements.txt
```

---

## ğŸ¤– LM Studio Setup

To run gemma-3-1b locally:

1. Install [LM Studio](https://lmstudio.ai/)
2. In LM Studio â†’ **Models** tab, search for **`google/gemma-3-1b`**
3. Download the GGUF (chat/instruct) version
4. Go to **Server** tab:

   * Toggle **OpenAIâ€‘compatible API**
   * Ensure itâ€™s running at `http://localhost:1234/v1`

---

## ğŸ” Streamlit Secrets Configuration

Create a `.streamlit/secrets.toml` file:

```toml
[tavily]
api_key = "your_tavily_api_key"

[llmstudio]
api_base = "http://localhost:1234/v1"
api_key = "lm-studio"
```

---

## âœ… Run the App

```bash
streamlit run app.py
```
---

## ğŸ” How It Works

Once the app is running:

1. ğŸ“ Enter your research topic
2. ğŸš€ Click **Start Research**

Under the hood:

* LangGraph breaks your topic into subâ€‘questions
* LM Studio (using **google/gemma-3-1b**) frames and expands ideas locally
* Tavily fetches upâ€‘toâ€‘date search results
* Results are organized by theme and credibility
* A clean research summary is synthesized
* You see all steps of the reasoning chain so you learn *how* the AI arrived at its conclusions

---

## ğŸ’» What You Need to Do

Before launching:

* âœ… Get your **Tavily API Key**
* âœ… Install **LM Studio**
* âœ… Download the **`google/gemma-3-1b`** GGUF model
* âœ… Enable **OpenAIâ€‘compatible API** in LM Studio (`http://localhost:1234/v1`)
* âœ… Add your `.streamlit/secrets.toml` config
* âœ… Run the app and input your topic

---

## ğŸ“š Use Cases

* Investigating controversial claims
* Breaking down complex questions
* Sourcing reliable information from the web
* Practicing the research process with AI assistance


---

## ğŸ¤ Contributing

Ideas, bugs, or feature requests? Open an issue or pull requestâ€”collaborators welcome!

---

## ğŸ§  Learn the Process, Not Just the Output

Tavilyâ€‘Gemma Research Helper doesnâ€™t just tell you whatâ€™s trueâ€”it walks you through *how* it figured it out.
Perfect for students, analysts, and anyone who wants AI that *teaches*, not just *tells*.
